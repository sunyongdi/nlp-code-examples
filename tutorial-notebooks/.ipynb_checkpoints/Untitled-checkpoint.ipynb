{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88090003-9b31-48df-abf1-2427a76f0b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b41b5d5-14b0-492d-8db3-23bfc1300ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    use_gpu = True\n",
    "    gpu_id = 0\n",
    "    preprocess = True\n",
    "    bert_path = '/root/pretrained/bert-base-chinese'\n",
    "    data_path = '/root/data/public/ChnSentiCorp'\n",
    "\n",
    "    # 预处理后存放文件位置\n",
    "    out_path = 'data/out'\n",
    "    max_len = 256\n",
    "    batch_size = 8\n",
    "    dropout = 0.3\n",
    "    num_hidden = 768\n",
    "    num_classes = 119\n",
    "    train_batch_size = 2\n",
    "    epoch = 1\n",
    "    seed = 1234\n",
    "    early_stopping_patience = 6\n",
    "    model_name = 'bert'\n",
    "    train_log = 10\n",
    "    log_interval = 10\n",
    "cfg = Config()\n",
    "cfg.cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3ce8c1b-dff2-4c4e-a764-4e6d23e002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU\n",
    "if cfg.use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device('cuda', cfg.gpu_id)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa50590-7ea5-4e34-9ea9-d663b1caadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子\n",
    "def manual_seed(seed: int = 1) -> None:\n",
    "    \"\"\"\n",
    "        设置seed。\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    #if torch.cuda.CUDA_ENABLED and use_deterministic_cudnn:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "manual_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d938308-cdfe-4795-aded-7e40f9f54676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataload 加载数据集\n",
    "sys.path.append('/root/projects/nlp-code-examples')\n",
    "from src.easy_bert.TextClassifiy import tools\n",
    "\n",
    "train_data_path = os.path.join('/root/projects/nlp-code-examples/example/TextClassifiy/data/', 'train.pkl')\n",
    "valid_data_path = os.path.join('/root/projects/nlp-code-examples/example/TextClassifiy/data/', 'dev.pkl')\n",
    "test_data_path = os.path.join('/root/projects/nlp-code-examples/example/TextClassifiy/data/', 'dev.pkl')\n",
    "\n",
    "train_dataset = tools.CustomDataset(train_data_path)\n",
    "valid_dataset = tools.CustomDataset(valid_data_path)\n",
    "test_dataset = tools.CustomDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac919a83-78c3-49bb-888e-2713fd51e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b7db1f-4d99-4e73-a5f1-61dc4bff0bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "class BasicBert(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(BasicBert, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(cfg.bert_path)\n",
    "        self.bert_drop = nn.Dropout(cfg.dropout)\n",
    "        self.out = nn.Linear(cfg.num_hidden, cfg.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, o2 = self.bert(**x, return_dict=False)\n",
    "        bo = self.bert_drop(o2)\n",
    "        output = self.out(bo)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59673d6-e8c2-461c-b6ce-9c67d43ba5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/pretrained/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BasicBert(cfg)\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f75242a7-11e5-4463-9c2c-c34e6520d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"] # 官方默认\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a339e381-9b67-437f-b912-90d4ef67ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "num_train_steps = int(len(train_dataset) / cfg.train_batch_size * cfg.epoch)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0164cbb-bfa4-4a3f-8e94-590cb18899fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1, best_epoch = -1, 0\n",
    "es_loss, es_f1, es_epoch, es_patience, best_es_epoch, best_es_f1, es_path, best_es_path = 1e8, -1, 0, 0, 0, -1, '', ''\n",
    "train_losses, valid_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccffadbc-cdb5-4d0a-8655-8abc7b169d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def trainer(dataloader, model, criterion, optimizer):\n",
    "    for batch_idx, (x, y) in enumerate(dataloader, 1):\n",
    "        for key, value in x.items():\n",
    "            x[key] = value.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad() # 梯度清0\n",
    "        y_pred = model(x)\n",
    "        # loss = F.cross_entropy(y_pred, y.long())\n",
    "        # loss = criterion(y_pred, y.long())\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 提督更新\n",
    "        losses.append(loss.item())\n",
    "        if batch_idx % 10 == 0:\n",
    "            # print('loss:', loss.item())\n",
    "            y_pred = torch.max(y_pred, 1)[1].cpu().numpy()\n",
    "            y_true = y.cpu().detach().numpy()\n",
    "            print(y_pred)\n",
    "            print(y_true)\n",
    "            p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', warn_for=tuple())\n",
    "            # print(f\"\"\"p:{p}, r:{r}, f1:{f1}\"\"\")\n",
    "    plt.plot(losses)\n",
    "    plt.title(f'epoch {1} train loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162475b-d653-42b8-8fd8-e3a0be4da41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54 70 56 25 48 70 83 53]\n",
      "[55.  2. 58. 25. 51. 53. 85. 53.]\n",
      "[116  34  17  46 106  92  36  65]\n",
      "[116.   9.  17.  46. 106. 117.  36.  65.]\n",
      "[17 17 71 17 49 70 24 35]\n",
      "[53. 17. 71. 17. 49. 82. 24. 35.]\n",
      "[ 28  48 106  22  28 106  70  54]\n",
      "[ 28.  48. 106.  22.  28. 107. 101.  59.]\n",
      "[17 13 95 53 34 16 20 53]\n",
      "[21. 13. 95. 70. 34. 16. 20. 53.]\n",
      "[70 25 71 18 18 46 88 17]\n",
      "[70. 25. 71. 17. 14. 46. 71. 17.]\n",
      "[ 70 102  36  70  44  70  95  70]\n",
      "[113.  70.  36.  69. 118.  70.  95.  70.]\n",
      "[ 96  71  88  20 106  83  70  24]\n",
      "[ 98.  42.   4.  20. 111.  85.  70.  18.]\n",
      "[70 17 70 17 17 17 97 53]\n",
      "[70. 17. 70. 71. 17. 17. 97. 31.]\n",
      "[17 95 94 18 53 95 36 34]\n",
      "[70. 95. 94. 18. 53. 95. 36. 28.]\n",
      "[ 8 17 21 95 70  8 17 45]\n",
      "[  8.  17.  21.  95. 103.   8.  17.  11.]\n",
      "[34 18 70 25 48 48 46 70]\n",
      "[34. 18. 70. 25. 48. 48. 46. 87.]\n",
      "[ 56 106 111  17  18  16  17  18]\n",
      "[ 56. 111. 106.  21.  21.  15.  17.  22.]\n",
      "[70 70 20 34 70 35 70 70]\n",
      "[70. 70. 20. 11. 70. 35. 70. 95.]\n",
      "[17 28 21 17 71 19 70 70]\n",
      "[ 17.  28.  21.  19.  70.  19. 101.  70.]\n",
      "[  8 102  70  18  70  28  34  49]\n",
      "[  8.  70.  70.  18. 118.  28.  34.  49.]\n",
      "[70 44 20 48 13 20 95 28]\n",
      "[ 70.  45.  20.  50.  13.  20. 105.  88.]\n",
      "[101  94 113  70  17  99  62  95]\n",
      "[101.  94. 113.  70.  17.  99.  62.  92.]\n",
      "[ 71  71  20  53  18  19  17 113]\n",
      "[ 38.  71.  14.  53.  18.  18.  18. 113.]\n",
      "[70 70 21 19 95 70 53 22]\n",
      "[70. 11. 21. 19. 95. 70. 53. 22.]\n",
      "[ 84 106  85  18  70  16  21  46]\n",
      "[  8. 106.  85.  18.  70.  16.  21.  46.]\n",
      "[96 70 70 24 65 49 95 17]\n",
      "[ 99.  11.  82. 106. 109.  49.  95.  17.]\n",
      "[ 70   9   2  22  13  64  21 101]\n",
      "[113.  70.   2.  22.  13.  69.  13. 101.]\n",
      "[113  24  70  46  49  45  28  18]\n",
      "[113.  24. 106.  46.  49.  45.  28.  18.]\n",
      "[ 16  22  18  17 106  17  99 106]\n",
      "[ 15.  22.  17.  17. 106.  17.  95. 111.]\n",
      "[ 22 102  91  36  70  70  17  71]\n",
      "[22. 70. 91. 36. 70. 70. 17. 71.]\n",
      "[ 85 113  71  59  94  95 106  17]\n",
      "[ 70.  70.  71.  59.  94.  95. 106.  17.]\n",
      "[ 70  70  95  70 106  99  59  54]\n",
      "[70. 70. 95. 70. 34. 99. 53. 54.]\n",
      "[ 92 101  78  71  16 113  18  17]\n",
      "[ 92. 101.  77.  71.  16. 113.  21.  17.]\n",
      "[96 70 25 28 70 53 18 70]\n",
      "[ 94.  70.  28. 118.  70.  53.  12.  26.]\n",
      "[ 19  70  71  70  13  13  70 106]\n",
      "[ 19.  70.  71.  70.  13.  12.  70. 111.]\n",
      "[22 95 96 78  9 48  9 70]\n",
      "[21. 95. 99. 78.  4. 51.  9. 70.]\n",
      "[106  70  70  53  64  18   9  53]\n",
      "[105.  70. 101.  70.  63.  18.   9.  53.]\n",
      "[ 36  53 106   9 106  71  71  21]\n",
      "[ 36.  53. 111.   9. 111.  71.  71.  21.]\n",
      "[70 24 35 36 56 34 21 97]\n",
      "[48. 24. 35. 36. 58. 34. 21. 97.]\n",
      "[ 97  13  70  70 102  70  82  24]\n",
      "[ 97.  21.  70.  70. 102.  70.  82.  24.]\n",
      "[88 81 70 17 49 71 70 92]\n",
      "[110.  81.  70.  19.  49.  17.  70. 117.]\n",
      "[ 70  18  21  71  34 106  70  96]\n",
      "[114.  15.  21.  71.  34. 111.  70.  96.]\n",
      "[95 70 71 17 17 17 20 70]\n",
      "[95. 42. 71. 17. 17. 17. 20. 70.]\n",
      "[111  36  28  16  70  64  16  70]\n",
      "[111.  34.  28.  16.  70.  64.  16.  70.]\n",
      "[ 21  53  78  21  94 102  20  36]\n",
      "[21. 53. 78. 21. 94. 70. 20. 36.]\n",
      "[96  9  8 17 17 54 36 78]\n",
      "[96.  9.  8. 17. 16. 54. 36. 70.]\n",
      "[70 21 16 70  8 17 70 25]\n",
      "[70. 21. 16. 24.  3. 16. 70. 28.]\n",
      "[46 71 21 17 17 56 70 34]\n",
      "[49. 71. 13. 17. 17. 56. 70. 34.]\n",
      "[10 95 91 13 70 70 71 20]\n",
      "[10. 95. 91. 13. 70. 48. 21. 20.]\n",
      "[ 18  20 111  20  20  19  97  25]\n",
      "[ 18.  20. 106.  20.  20.  19.  97.  25.]\n",
      "[ 36  16  70  17  62  21  19 103]\n",
      "[ 36.  15.  70.  17.   8.  17.  19. 103.]\n",
      "[  8  70  17  70 106  70  70  70]\n",
      "[  8.  70.  17.  70. 111.  70.  70.  28.]\n",
      "[17 70 10 70 34 96 18 36]\n",
      "[17. 70.  7. 71. 41. 96. 18. 36.]\n",
      "[ 94  50  45  56  71  13  71 106]\n",
      "[ 94.  50.  45.  53.  71.  23.  71. 110.]\n",
      "[18 70 83 71 28 71 70 62]\n",
      "[21. 11. 83. 71. 25. 71. 70. 36.]\n",
      "[36  8 17 70 44 72 94 85]\n",
      "[36. 78. 17. 70. 27. 84. 94. 85.]\n",
      "[ 56  70  18  17  25  36  17 106]\n",
      "[ 60. 114.  21.  17.  49.  36.  20. 116.]\n",
      "[ 25  21  17  34  17  75  70 106]\n",
      "[25. 13. 17. 96. 18. 75. 33. 28.]\n",
      "[17 48 70 59 18 22 48  1]\n",
      "[71. 70. 70. 34. 18. 18. 48.  1.]\n",
      "[ 4 48 96 62 20 70 18 53]\n",
      "[70. 48. 99. 63. 20. 70. 13. 53.]\n",
      "[ 17 106  18  93  70  70 102  18]\n",
      "[ 17. 106.  12.  93.  70.  70. 102.  18.]\n",
      "[ 48  70  70 111 106  22  71  18]\n",
      "[ 48.  70.  70.   4. 106.  22.  71.  13.]\n",
      "[ 17  90  70  46  21  19 104  71]\n",
      "[ 21.  90.  70.  46.  21.  19. 104.  71.]\n",
      "[96 59  8 48 85 17 70 95]\n",
      "[96. 53.  8. 17. 85. 70. 70. 95.]\n",
      "[ 18  70 111  22  19  70  70  20]\n",
      "[ 17.  70. 111.  22.  18.  70.  81.  20.]\n",
      "[17 77 70 20 54 71 28 96]\n",
      "[17. 77. 70. 20. 54. 17. 47. 96.]\n",
      "[46 34 13 28 70 46 53 28]\n",
      "[46. 34. 13. 28. 46. 49. 53. 40.]\n",
      "[ 46  71  70  17  70  70 106  95]\n",
      "[ 46.  17.  70.  21.  70.  70. 116.  95.]\n",
      "[17 70 19 34  4 71 36 22]\n",
      "[21. 70. 19. 33. 28. 71. 36. 18.]\n",
      "[ 20  70 106  71  19  71  95  16]\n",
      "[ 20.  70. 111.  71.  19.  71.  95.  18.]\n",
      "[ 70  16  70  85  71  91 103  18]\n",
      "[ 70.  16. 103.  84.  71.  85. 103.  18.]\n",
      "[71 54 70 18 49 91 58 36]\n",
      "[71. 54. 70. 17. 49. 91. 58. 18.]\n",
      "[91 17 25 85 96 36 77 76]\n",
      "[91. 17. 32. 84. 96. 36. 77. 76.]\n",
      "[ 94 106  70  21  94   4  88  24]\n",
      "[ 94.  70.  70.  21.  94. 113.  88.  24.]\n",
      "[53 21 56 19  9 45 95  9]\n",
      "[53. 21. 56. 17.  9. 96. 95. 10.]\n",
      "[ 53  17  70  28 106  70  18  70]\n",
      "[ 53.  17.  26.  28. 106.  70.  18.  70.]\n",
      "[ 59  54  16  46  70  54  48 101]\n",
      "[85. 54. 18. 46. 70. 54. 48. 46.]\n",
      "[ 45  17  18 106  70  70  70  17]\n",
      "[ 11.  71.  18. 111.  41.  70.  70.  17.]\n",
      "[92 85 54 96 16  8 70 18]\n",
      "[92. 83. 61. 96. 16.  8. 70. 18.]\n",
      "[70 70 48 70 70 53 73 24]\n",
      "[70. 70. 48. 70. 70. 38. 73. 28.]\n",
      "[ 70  53  59  59  70  91 106  49]\n",
      "[24. 46. 57. 49. 70. 91. 99. 46.]\n",
      "[70 56 17 25 71 22 70 36]\n",
      "[70. 56. 17. 29. 71. 23. 70. 42.]\n",
      "[70 70 53 95 28 46 70 93]\n",
      "[24. 70. 53. 95. 28. 46. 28. 93.]\n",
      "[70 17 36 70 49 99 70 59]\n",
      "[70. 18. 36. 70. 49. 95. 70. 59.]\n",
      "[ 70  19  11  36  17  71  36 111]\n",
      "[ 1. 19. 11. 70. 17. 71. 34. 11.]\n",
      "[ 70  70 111  19 106   0  17  94]\n",
      "[ 70.  70. 111.  19. 106.   0.  17.  94.]\n"
     ]
    }
   ],
   "source": [
    "trainer(train_dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "98d41bcb-0250-4725-a348-221f8cf724bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b30acbe-3a8b-4f5c-9cbc-d7c59290e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ef6b808-d373-48f4-b99a-4031cdb971a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', warn_for=tuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb36614b-a56a-42a1-838e-8b95bb113cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, acc, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', warn_for=tuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3239f24e-85d6-4b2a-96a4-bc67f0883bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ed6192b-37db-4fcf-9ca6-8c41b4fa02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85ba3874-447d-4b67-9aee-2600f1f1e381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46875"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "99f84a51-5623-4f3e-b2a4-2477d7e3c697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7974, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e5f0f-3aa6-4de7-b6d4-d919d7865c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

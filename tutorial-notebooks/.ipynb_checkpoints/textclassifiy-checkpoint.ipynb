{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f396ca64-0a43-4ce1-a1ea-db0d5b54b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from typing import NewType, List, Tuple, Dict, Any\n",
    "\n",
    "# myself\n",
    "sys.path.append('/root/projects/nlp-code-examples')\n",
    "from src.easy_bert.TextClassifiy import models\n",
    "from src.easy_bert.TextClassifiy import tools\n",
    "from src.easy_bert.TextClassifiy import utils\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "211cc56a-5381-46e0-8d47-737e4b084cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    use_gpu = True\n",
    "    gpu_id = 0\n",
    "    preprocess = True\n",
    "    bert_path = '/root/pretrained/bert-base-chinese'\n",
    "    data_path = '/root/data/public/ChnSentiCorp'\n",
    "\n",
    "    # 预处理后存放文件位置\n",
    "    out_path = 'data/out'\n",
    "    max_len = 256\n",
    "    batch_size = 32\n",
    "    dropout = 0.3\n",
    "    num_hidden = 768\n",
    "    num_classes = 1\n",
    "    train_batch_size = 32\n",
    "    epoch = 1\n",
    "    seed = 1234\n",
    "    early_stopping_patience = 6\n",
    "    model_name = 'bert'\n",
    "    train_log = 10\n",
    "    log_interval = 10\n",
    "cfg = Config()\n",
    "cfg.cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9874a559-0a2a-4329-a5c0-89d4cf8824dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device('cuda', cfg.gpu_id)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef659cb-8061-449e-b939-5987492cab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "tools.preprocess(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d072d89f-ef98-40aa-a6ca-7b6ccac91f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(cfg.cwd, cfg.out_path, 'train.pkl')\n",
    "valid_data_path = os.path.join(cfg.cwd, cfg.out_path, 'valid.pkl')\n",
    "test_data_path = os.path.join(cfg.cwd, cfg.out_path, 'test.pkl')\n",
    "\n",
    "train_dataset = tools.CustomDataset(train_data_path)\n",
    "valid_dataset = tools.CustomDataset(valid_data_path)\n",
    "test_dataset = tools.CustomDataset(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c80b802f-c5a8-422e-ad52-388182d76b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=True, collate_fn=tools.collate_fn(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35550e8a-a11c-4695-9225-ee42eba49edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/pretrained/bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ffbcdb0-bb90-4d8d-bc57-3c437df94b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "cfg.dataset_len = len(train_dataset)\n",
    "num_train_steps = int(cfg.dataset_len / cfg.train_batch_size * cfg.epoch)\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "best_f1, best_epoch = -1, 0\n",
    "es_loss, es_f1, es_epoch, es_patience, best_es_epoch, best_es_f1, es_path, best_es_path = 1e8, -1, 0, 0, 0, -1, '', ''\n",
    "train_losses, valid_losses = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27bce652-d821-467c-a5c2-f259e892dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = None\n",
    "for epoch in range(1, cfg.epoch + 1):\n",
    "    utils.manual_seed(cfg.seed + epoch)\n",
    "    train_loss = tools.train(epoch, model, train_dataloader, optimizer, scheduler, criterion, device, writer, cfg)\n",
    "    valid_f1, valid_loss = tools.validate(epoch, model, valid_dataloader, criterion, device, cfg)\n",
    "    # scheduler.step(valid_loss)\n",
    "    model_path = model.save(epoch, cfg)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    if best_f1 < valid_f1:\n",
    "        best_f1 = valid_f1\n",
    "        best_epoch = epoch\n",
    "    # 使用 valid loss 做 early stopping 的判断标准\n",
    "    if es_loss > valid_loss:\n",
    "        es_loss = valid_loss\n",
    "        es_f1 = valid_f1\n",
    "        es_epoch = epoch\n",
    "        es_patience = 0\n",
    "        es_path = model_path\n",
    "    else:\n",
    "        es_patience += 1\n",
    "        if es_patience >= cfg.early_stopping_patience:\n",
    "            best_es_epoch = es_epoch\n",
    "            best_es_f1 = es_f1\n",
    "            best_es_path = es_path\n",
    "            \n",
    "if best_es_path == '':\n",
    "    best_es_path = es_path\n",
    "            \n",
    "_ , test_loss = tools.validate(-1, model, test_dataloader, criterion, device, cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.8",
   "language": "python",
   "name": "torch1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
